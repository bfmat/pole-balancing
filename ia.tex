\documentclass[12pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{parskip}
\usepackage{amsmath}
\usepackage[style=ieee]{biblatex}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{caption}

\graphicspath{{./images/}}

\addbibresource{ia.bib}

\begin{document}

\title{Applying State Space Control and LQR to the Pole Balancing Problem}
\author{Brendon Matusch}
\date{November 2018}
\maketitle

\section{Introduction}

\subsection{The Pole Balancing Problem}

One of the most famous problems in control theory is the \textit{pole balancing problem}, also known as \textit{cart pole}. The basic idea of the problem is as follows:

\begin{itemize}
    \item A movable cart with mass $M$ sits on a two-dimensional, straight track, at horizontal position $x$.
    \item A pole with length $l$ is anchored to the cart with a hinge at angle $\theta$ (which can change).
    \item A weight with mass $m$ is attached to the end of the pole.
    \item A motor can accelerate the cart to the left or right with force $F$.
    \item Gravity, which applies downward force $g$, is present.
    \item The goal is to prevent the pole from falling over (to prevent $\theta$ from becoming too large), while also keeping the cart near the center of the track (keeping $x _c$ small).
\end{itemize}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{cartpole}
    \caption{\label{cartpole} A diagram of the pole balancing problem.}
\end{figure}

This problem, as visualized in Figure \ref{cartpole}, is quite clearly not trivial. If the pole starts to fall to the right, one must accelerate the cart to the right to prevent it from falling over. However, that also results in the cart moving away from the center. Thus, to successfully optimize both the angle and the position, one must gradually adjust the angle by applying small forces while the cart moves.

\subsection{Control Algorithms}

The most common classical control solution to this kind of problem is the PID (proportional, integral, and derivative) closed control loop. While it is well-known that this can work, it is relatively difficult to optimize. PID loops are only able to optimize one variable at a time (for instance, the angle of the pole, or the position of the cart). Optimizing multiple variables (as is required here) necessitates nesting of PID loops, which frequently have to be optimized either manually or via machine learning.

An alternative control method, which can optimize multiple variables with one or more inputs, is state space control. Rather than dealing with only a single variable, state space controllers can optimize an entire vector of states in a single linear function, making for a simpler and more elegant solution.

For this project, I have created a mathematical simulation of the pole balancing problem and developed a state space controller to effectively optimize it. While this was a challenging endeavor and I had to learn a lot of new mathematics, I am very happy with the result.

\subsection{Applications}

A state space solution to the pole balancing problem has many applications outside of this environment. For instance, control of many kinds of robots (such as two-wheeled vehicles and walking humanoid robots) is related to the pole balancing problem. I have learned a great deal along the way about how state space control algorithms can generalize onto real-world control problems.

\section{Environment}

\subsection{Equations of Motion} \label{equations}

In order to develop a solution to the pole-balancing problem, I first had to model it. Creating a physical cart pole setup would have taken a long time, so I used a simulated model instead. In order to develop this, I used the differential equations of motion for the pole balancing problem, which I found at \cite{polebalancing}:

\begin{equation}
    \ddot \theta = \frac{\displaystyle g \sin \theta + \cos \theta \left( \frac{\displaystyle -F - m l \dot \theta ^2 \sin \theta}{\displaystyle M + m} \right)}{\displaystyle l \left( \frac{\displaystyle 4}{\displaystyle 3} - \frac{\displaystyle m \cos ^2 \theta}{\displaystyle M + m} \right)}
\end{equation}

\begin{equation}
    \ddot x _c = \frac{\displaystyle F + m l \left( \dot \theta ^2 \sin \theta - \ddot \theta \cos \theta \right)}{\displaystyle M + m}
\end{equation}

Note the use of dot-derivative notation, where $\displaystyle \dot \theta = \frac{\displaystyle d \theta}{\displaystyle d t}$ and $\displaystyle \ddot \theta = \frac{\displaystyle d ^2 \theta}{\displaystyle d t ^2}$ (and respectively for $x _c$).

\subsection{Implementation}

I implemented these equations of motion in Python 3 using NumPy \cite{numpy}. The following constants were used:

\begin{itemize}
    \item $M = 1$
    \item $m = 0.1$
    \item $l = 0.5$
    \item $g = 9.81$
\end{itemize}

The differential equations of motion refer to a continuous environment, where acceleration, position, and velocity values change every instant. Since this is impossible to compute in reality, I implemented the system with a time step of 0.001 (meaning that all variables of motion are updated every 0.001 time units).

\subsection{Visualization}

\subsubsection{Graphical Front-End}

To make the model more easy to visualize and understand, I developed a graphical front-end using PyQt5 (Figure \ref{simulation}). It displays the cart and pole as rectangles on top of the linear track.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{simulation}
    \caption{\label{simulation} A screenshot of the graphical front-end.}
\end{figure}

\subsubsection{Motion Plots}

For more precise static visualization of overshooting and other potential problems with control algorithms, I used Matplotlib \cite{matplotlib} to create plots over time of $x _c$, $\theta$, and $F$. These plots are seen throughout the rest of this paper.

\section{State Space}

\subsection{Concepts}

I learned about state space control from a book \cite{veness} written by Tyler Veness, a FIRST Robotics Competition mentor. I used the knowledge I gained in this book to develop a state space control algorithm that works for the pole balancing problem. The basic concepts of state space control are as follows:

\begin{enumerate}
    \item States of the environment to be controlled are expressed in a constantly changing state vector $x$ (not to be confused with the horizontal position $x _c$).
    \item Inputs to the system are provided as an input vector $u$.
    \item The derivative of the state vector, $\dot x$, is defined by the following equation:
        \begin{equation}
            \dot x = A x + B u
        \end{equation}
        The two matrices used in this equation represent a linear model of the environment as follows:
        \begin{itemize}
            \item The system matrix $A$ with size (states $\times$ states) defines how the states automatically change over time (in dynamics such as gravity).
            \item The input matrix $B$ with size (states $\times$ inputs) defines how the inputs provided by a control algorithm influence the changing states of the environment.
        \end{itemize}
    \item The basic control law of state space is as follows:
        \begin{equation} \label{control_law}
            u = K(r - x)
        \end{equation}
        The \textit{reference} vector, which contains the values we hope to drive the states to, is written as $r$. If the goal is to minimize the states, it will be a vector of zeroes. $K$ is the \textit{controller gain} matrix, of size (inputs $\times$ states). It is multiplied by the state error $r - x$ to produce a (hopefully) optimal input to drive the states to the reference.
\end{enumerate}

\subsection{Implementation}

\subsubsection{States and Inputs}

The first step in developing a state space control algorithm was to express the environment in state space notation. This means choosing a vector of states and a vector of inputs that fully define the cart pole system.

The physical dynamics can be fully described with only four parameters: $\theta$, $\dot \theta$, $x _c$, and $\dot x _c$. These are the only non-constant parameters used in the differential equations of motion in Section \ref{equations}. Thus:

\begin{equation}
    \displaystyle x = \begin{bmatrix}
        \theta \\
        \dot \theta \\
        x _c \\
        \dot x _c
    \end{bmatrix}
\end{equation}

While there are four states required to describe the environment, there is only one parameter the controller has access to, and that is the force applied to the cart. Thus:

\begin{equation}
    u = \begin{bmatrix}
        \displaystyle F
    \end{bmatrix}
\end{equation}

\subsubsection{Linearization}

Since the full behavior of the environment is described in two constant matrices ($A$ and $B$), and matrix multiplication is a linear operation, the environment must actually be linear to use standard state space control.

Thankfully, I realized that $\theta$ will always be small (otherwise, the cart would either fall over, or have to constantly accelerate very rapidly). If $\theta$ is small, the nonlinear trigonometric ratios can be approximated as linear functions with minimal error. Specifically, when $\theta$ is small, $\sin \theta \approx \theta$ and $\cos \theta \approx 1$. I took advantage of these facts to linearize the pole balancing problem, as follows:

\begin{itemize}
    \item The torque applied by gravity on the arm is equal to $\displaystyle \frac{\displaystyle g \sin \theta}{\displaystyle l}$, which can be linearly approximated as $\displaystyle \frac{\displaystyle g \theta}{\displaystyle l}$ for values of $\theta$ near 0.
    \item The torque applied by horizontal acceleration of the cart on the arm (assuming the arm is of negligible mass) is equal to $\displaystyle - \frac{\displaystyle F \cos \theta}{\displaystyle l}$, which can be linearly approximated as $\displaystyle - \frac{\displaystyle F}{\displaystyle l}$ for values of $\theta$ near 0.
    \item The acceleration applied to the cart by a force is simply equal to $\displaystyle \frac{\displaystyle F}{\displaystyle M + m}$ (once again, under the assumption that the pole mass is negligible).
    \item The only dynamics other than gravity and force that must be modeled are the influences of the velocity states $\dot \theta$ and $\dot x _c$ on $\theta$ and $x _c$, respectively. These are obviously linear.
\end{itemize}

Thus, we have the following five linearized equations of motion, which define the changing state over one unit of time:

\begin{equation}
    \displaystyle \theta := \theta + \dot \theta
\end{equation}

\begin{equation}
    \displaystyle \dot \theta := \dot \theta - \frac{\displaystyle F \cos \theta + g \theta}{\displaystyle l}
\end{equation}

\begin{equation}
    \displaystyle x _c := x _c + \dot x _c
\end{equation}

\begin{equation}
    \dot x _c := \displaystyle \dot x _c + \frac{\displaystyle F}{\displaystyle M + m}
\end{equation}

These linear equations very nicely map onto the following linear algebra equation (once again, over one time step):

\begin{equation}
    \displaystyle
    \begin{bmatrix}
        \theta \\
        \dot \theta \\
        x _c \\
        \dot x _c
    \end{bmatrix}
    := \begin{bmatrix}
        \theta \\
        \dot \theta \\
        x _c \\
        \dot x _c
    \end{bmatrix}
    + \begin{bmatrix}
        0 & 1 & 0 & 0 \\
        \frac{g}{l} & 0 & 0 & 0 \\
        0 & 0 & 0 & 1 \\
        0 & 0 & 0 & 0
    \end{bmatrix}
    \begin{bmatrix}
        \theta \\
        \dot \theta \\
        x _c \\
        \dot x _c
    \end{bmatrix}
    + \begin{bmatrix}
        0 \\
        - \frac{1}{l} \\
        0 \\
        \frac{1}{M + m}
    \end{bmatrix}
    \begin{bmatrix}
        F
    \end{bmatrix}
\end{equation}

Thus, we have our values for $A$ and $B$:

\begin{equation}
    \displaystyle A = \begin{bmatrix}
        0 & 1 & 0 & 0 \\
        \frac{g}{l} & 0 & 0 & 0 \\
        0 & 0 & 0 & 1 \\
        0 & 0 & 0 & 0
    \end{bmatrix}
\end{equation}

\begin{equation}
    \displaystyle B = \begin{bmatrix}
        0 \\
        - \frac{1}{l} \\
        0 \\
        \frac{1}{M + m}
    \end{bmatrix}
\end{equation}

\section{Control}

\subsection{Objective}

While the model of the environment is complete, we currently have no way to control it. To use the basic state space control law in Equation \ref{control_law}, we need the controller gain matrix $K$ and the reference $r$. Our objective is to find values of $K$ and $r$ such that the pole will remain in a stable, upright state, and reach it as quickly as possible with minimal error.

\subsection{Reference}

The reference vector $r$ is easy to find: in the pole balancing problem, one usually wants to bring the angle, angular speed, position, and positional speed all to zero. This is a stable state, and centers the cart within the bounds of the track. Thus, we simply define $r$ as a vector of zeroes:

\begin{equation}
    \displaystyle
    r = \begin{bmatrix}
        0 \\
        0 \\
        0 \\
        0
    \end{bmatrix}
\end{equation}

\subsection{Controller Gain Matrix}

\subsubsection{Concept}

Finding the controller gain matrix $K$ is clearly the most difficult part of developing a state space controller. In Equation \ref{control_law}, we can see that it is a matrix mapping the error of the current states off the reference to the input $u$. More intuitively, it can be thought of as a linear regression of the states, used to calculate the best possible input.

\subsubsection{Linear-Quadratic Regulator}

Rather than being trained as in a typical linear regression, an optimal value for $K$ can be found using a control law called Linear-Quadratic Regulator (LQR). Like a linear regression, LQR optimizes a cost function. However, rather than requiring training data extracted from a model of the environment, LQR can optimize $K$ using only the state-space model matrices $A$ and $B$, alongside two cost matrices defined by the developer.

The cost function optimized is as follows:

\begin{equation}
    J = \int _0 ^{\infty} \! (x ^T Q x + u ^T R u) \, dt
\end{equation}

$Q$ are diagonal matrices that define the costs of large errors for each of the states respectively. $R$ defines the cost of large values for each of the inputs. After using Bryson's rule \cite{veness}, and some empirical optimization, I settled on the following values for Q and R:

\begin{equation}
    Q = 40 \begin{bmatrix}
        1 & 0 & 0 & 0 \\
        0 & \frac{1}{9} & 0 & 0 \\
        0 & 0 & \frac{1}{100} & 0 \\
        0 & 0 & 0 & \frac{1}{400}
    \end{bmatrix}
\end{equation}

\begin{equation}
    R = \begin{bmatrix}
        \displaystyle
        \frac{1}{100}
    \end{bmatrix}
\end{equation}

Given values for $Q$ and $R$, the optimal controller gain matrix $K$ was calculated using the Python Control library.

\printbibliography

\end{document}